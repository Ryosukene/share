{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d140e889",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mexpert\u001b[39;00m\n\u001b[1;32m      3\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(expert\u001b[38;5;241m.\u001b[39mpy)\n",
      "File \u001b[0;32m~/refine/s3prl/downstream/speaker_linear_utter_libri/expert.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#-------------#\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpeakerDataset\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDownstreamExpert\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import expert\n",
    "importlib.reload(expert.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*- #\n",
    "\"\"\"*********************************************************************************************\"\"\"\n",
    "#   FileName     [ dataset.py ]\n",
    "#   Synopsis     [ the libri speaker dataset ]\n",
    "#   Author       [ S3PRL ]\n",
    "#   Copyright    [ Copyleft(c), Speech Lab, NTU, Taiwan ]\n",
    "\"\"\"*********************************************************************************************\"\"\"\n",
    "\n",
    "\n",
    "###############\n",
    "# IMPORTATION #\n",
    "###############\n",
    "import os\n",
    "import random\n",
    "#-------------#\n",
    "import pandas as pd\n",
    "#-------------#\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.dataset import Dataset\n",
    "#-------------#\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "HALF_BATCHSIZE_TIME = 2000\n",
    "SPEAKER_THRESHOLD = 0\n",
    "\n",
    "\n",
    "###################\n",
    "# Speaker Dataset #\n",
    "###################\n",
    "class SpeakerDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, split, bucket_size, libri_root, split_file, bucket_file, sample_rate=16000, train_dev_seed=1337, **kwargs):        \n",
    "        \n",
    "        self.libri_root = libri_root\n",
    "        self.split_file = split_file\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        # Read table for bucketing\n",
    "        assert os.path.isdir(bucket_file), 'Please first run `preprocess/generate_len_for_bucket.py to get bucket file.'\n",
    "        self.table = pd.read_csv(os.path.join(bucket_file, 'train-clean-100.csv')).sort_values(by=['length'], ascending=False)\n",
    "        X = self.table['file_path'].tolist()\n",
    "        X_lens = self.table['length'].tolist()\n",
    "        \n",
    "        if (split == 'train' or split == 'dev') and os.path.isfile(os.path.join(split_file, 'train_split.txt')):\n",
    "            usage_list = open(os.path.join(split_file, 'train_split.txt')).readlines()\n",
    "            random.seed(train_dev_seed)\n",
    "            random.shuffle(usage_list)\n",
    "            percent = int(len(usage_list)*0.9)\n",
    "            usage_list = usage_list[:percent] if split == 'train' else usage_list[percent:]\n",
    "        elif split == 'test' and os.path.isfile(os.path.join(split_file, 'test_split.txt')):\n",
    "            usage_list = open(os.path.join(split_file, 'test_split.txt')).readlines()\n",
    "        else:\n",
    "            raise NotImplementedError('Invalid `split` argument!')\n",
    "        usage_list = {line.strip('\\n'):None for line in usage_list}\n",
    "\n",
    "        # Use bucketing to allow different batch sizes at run time\n",
    "        self.X = []\n",
    "        batch_x, batch_len = [], []\n",
    "\n",
    "        for x, x_len in zip(X, X_lens):\n",
    "            if self._parse_x_name(x) in usage_list: # check if x is in list\n",
    "                batch_x.append(x)\n",
    "                batch_len.append(x_len)\n",
    "                \n",
    "                # Fill in batch_x until batch is full\n",
    "                if len(batch_x) == bucket_size:\n",
    "                    # Half the batch size if seq too long\n",
    "                    if (bucket_size >= 2) and (max(batch_len) > HALF_BATCHSIZE_TIME):\n",
    "                        self.X.append(batch_x[:bucket_size//2])\n",
    "                        self.X.append(batch_x[bucket_size//2:])\n",
    "                    else:\n",
    "                        self.X.append(batch_x)\n",
    "                    batch_x, batch_len = [], []\n",
    "        \n",
    "        # Gather the last batch\n",
    "        if len(batch_x) > 1:\n",
    "            if self._parse_x_name(x) in usage_list: # check if x is in list if list not empty\n",
    "                self.X.append(batch_x)\n",
    "\n",
    "        # Compute speaker dictionary\n",
    "        print('[Dataset] - Computing speaker class...')\n",
    "        speakers = self._get_all_speakers(X)\n",
    "        self.speaker2idx = self._compute_speaker2idx(speakers)\n",
    "        self.class_num = len(self.speaker2idx)\n",
    "        print('[Dataset] - # possible speaker classes: ' + str(self.class_num) + ', number of data for ' + split + ': ' + str(len(usage_list)))\n",
    "\n",
    "    def _parse_x_name(self, x):\n",
    "        return x.split('/')[-1].split('.')[0]\n",
    "\n",
    "    def _load_wav(self, wav_path):\n",
    "        path=os.path.join(self.libri_root, feat_path)\n",
    "        wav=torch.load(path)\n",
    "        wav=torch.tensor(wav,dtype=torch.float)\n",
    "        # assert sr == self.sample_rate, f'Sample rate mismatch: real {sr}, config {self.sample_rate}'\n",
    "        return wav.view(-1)\n",
    "\n",
    "    def _get_speaker_from_path(self, x):\n",
    "        return x.split('/')[-1].split('.')[0].split('-')[0]\n",
    "\n",
    "    def _get_all_speakers(self, X):\n",
    "        speaker_set = {}\n",
    "        for x in X:\n",
    "            speaker = self._get_speaker_from_path(x)\n",
    "            if speaker not in speaker_set:\n",
    "                speaker_set[speaker] = 0\n",
    "            else:\n",
    "                speaker_set[speaker] += 1\n",
    "        return speaker_set\n",
    "\n",
    "    def _compute_speaker2idx(self, speakers):\n",
    "        idx = 0\n",
    "        speaker2idx = {}\n",
    "        for speaker in sorted(speakers):\n",
    "            if speaker not in speaker2idx and speakers[speaker] > SPEAKER_THRESHOLD: # eliminate the speakers with too few utterance\n",
    "                speaker2idx[speaker] = idx\n",
    "                idx += 1\n",
    "        return speaker2idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Load acoustic feature and pad\n",
    "        wav_batch = [self._load_wav(x_file) for x_file in self.X[index]]\n",
    "        label_batch = torch.LongTensor([self.speaker2idx[self._get_speaker_from_path(x_file)] for x_file in self.X[index]])\n",
    "        return wav_batch, label_batch # bucketing, return ((wavs, labels))\n",
    "\n",
    "    def collate_fn(self, items):\n",
    "        return items[0][0], items[0][1] # hack bucketing, return (wavs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64009289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path = \"./_expert.py\"\n",
    "\n",
    "os.path.exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a14d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
